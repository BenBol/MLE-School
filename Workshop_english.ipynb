{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you run this notebook in GOOGLE COLAB, you have to uncomment and run the following commands once!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# import sys\n",
    "# drive.mount('/content/gdrive')\n",
    "# !git clone https://github.com/BenBol/MLE-School.git\n",
    "# sys.path.append('MLE-School')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MNIST Beispiel](03_Images/SummerSchool22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this image (and the following) is not displayed, adjust the path with `'MLE-School/...'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of handwritten numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Moin\" and welcome to this interactive session in which we will use neural networks to identify handwritten letters and numbers.\n",
    "\n",
    "**Introduction:**\n",
    "- You should modify the marked parts of the program and you can execute the function section directly. To do this, add the code between ### start ### and ### end ###.\n",
    "- To execute a function section, you should select it and then press `STRG` + `ENTER`.\n",
    "- Python 3 is used for this course\n",
    "\n",
    "**Motivation for neural networks:**\n",
    "\n",
    "The more data is available, the greater is the advantage of neural networks over traditional machine learning approaches. \n",
    "\n",
    "- Rapid increase in the number\n",
    "    - networked devices and\n",
    "    - generated data (in the multiple zeta-bite range per year). \n",
    "    - Size of the data (e.g. accelerators / huge amounts of data that have to be pre-filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to train neural networks with open source libraries. In Python, Sciki-Learn is an entry-level variant.<br> \n",
    "More possibility in the design of neural networks is offered by [TensorFlow](https://www.tensorflow.org), which we will use in the following.\n",
    "<br><br>\n",
    "[TensorFlow](https://www.tensorflow.org) is an easy-to-use but powerful deep learning library for Python. In this course, we will, create a feed-forward neural network and train it to recognize handwritten characters.  <br>\n",
    "\n",
    "The dataset we will use for this task is also publicly available and is called [MNIST](http://yann.lecun.com/exdb/mnist/).<br><br>\n",
    "![MNIST Example](03_Images/Beispiel1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of relevant Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the beginning of each project the most important packages are imported, which are needed in the further course. This includes [Matplotlib](https://matplotlib.org) to generate graphics, [Numpy](https://numpy.org) to perform vector calculations and of course the [Keras](https://keras.io) package from [TensorFlow](https://www.tensorflow.org) to create the neural networks.\n",
    "\n",
    "**Task:** Run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for vector calculation\n",
    "import numpy as np\n",
    "\n",
    "# import for graphics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tensorflow for neural network creation\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally we provide small helper functions, which are located in the Python file: `Additional_Functions.py`.\n",
    "\n",
    "**Task:** import all functions by executing the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Additional_Functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [MNIST](http://yann.lecun.com/exdb/mnist/) dataset is generated from the much larger [NIST](https://www.nist.gov/srd/nist-special-database-19) dataset from 1995.   For this purpose, original black and white images of the numbers 0-9 were normalized to 20x20 pixels. (Hence, slight gray scales are visible).\n",
    "The images were then centered as image of 28x28 pixles by calculating the center of mass of the pixels and moving the image so that this point was in the center of the 28x28 field.\n",
    "\n",
    "The dataset is stored in the folder `./01_Dataset/mnist.npz` and can be loaded with the function ``mnist_data, mnist_label = load_data(path)``.\n",
    "\n",
    "**Task**: Load the dataset as described above.<br>\n",
    "**Reminder**: Fill the blancs between `### start ###` and `### end ###` and change `./01_Dataset/mnist.npz` to `'MLE-School/01_Datensatz/mnist.npz'` when using **COLAB**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### start ####\n",
    "path = ...\n",
    "mnist_data, mnist_label = ...\n",
    "### end ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When loading the dataset, two NumPy arrays are returned.</br>\n",
    "* The variable `mnist_data` contains the individual images. \n",
    "* The varialbe `mnist_label` consists of the numeric digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first take a closer look at the data set together.\n",
    "* With `type(variable)` the type of the variable can be displayed. \n",
    "* and `len(variable)` gives the number of entries\n",
    "\n",
    "**Task**: Test both commands to determine the type and length of both variables and check if the number of labels corresponds to the number of data points. (Reminder: with the `print(...)` command multiple outputs of a cell can be printed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### start ####\n",
    "...\n",
    "...\n",
    "### end ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Numpy library has implemented the `variable.shape` attribute, which gives more precise information about the dimensions of the vectors.<br>\n",
    "**Task:** Use the `shape` attribute for both variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### start ####\n",
    "dim_data = ...\n",
    "dim_label = ...\n",
    "print('Dimensions dataset: {}\\nDimensions label: {}'.format(dim_data, dim_label))\n",
    "### end ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the data set consists of 70000 digits, each with 28x28 pixel and the corresponding number in the label vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustrate the individual numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the individual example, try the Matplotlib function `plt.imshow(dataset)`. To get a single number from the data set, it can be selected by the \"slicing\" with `dataset[index]`.<br>\n",
    "**Task**: Plot some numbers for the `mnist_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### start ####\n",
    "...\n",
    "### end ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide here a function to plot multiple random images.<br>\n",
    "**Task**: use the function `plot_numbers(n_rows, n_columns, data, label)` to plot a grid of 4x5 numbers.<br>\n",
    "**Note**:  Since the numbers are shown randomly, you can run this function more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### start ###\n",
    "...\n",
    "### end ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, the color values of the numbers are examined more closely. For this purpose, a random number and additionally a histogram are displayed, in which the distribution of the color values is shown. <br><br>This line can also be executed more often to view different numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = plot_numbers(1,1,mnist_data, mnist_label)\n",
    "plt.hist(mnist_data[index].reshape(784), bins=20)\n",
    "plt.title(\"Distribution of pixel values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, most pixels have an intensity of zero with the exception of the dark number.\n",
    "Black is defined as 255, as usual. A few pixels have a gray value, which results from the anti-aliasing procedure for scaling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of individual numbers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, it is interesting to see how many images of each number are present in the data set. For this purpose, the frequency of the respective classes is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Number, frequency_in_dataset = np.unique(mnist_label, return_counts=True)\n",
    "plt.bar(Number, frequency_in_dataset)\n",
    "plt.xlabel('Number in the image')\n",
    "plt.ylabel('Quantity in the data set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, not all numbers are present with the same frequency. In case of strong deviations, this could be considered further. For this exercise, this is not necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training of neural networks with *dense layers* it is advantageous if the images are not in the form of a 28x28 vector but in the form of a 784x1 vector.\n",
    "\n",
    "With the function `Vector_new = Vector.reshape((b,c))` a matrix with the dimensions `Vector.shape == 'a,b'` can be transformed into `Vector_new.shape == 'b,c'`.\n",
    "Of course, the dimensions must fit.\n",
    "<img src=\"03_Images/Reshape.svg\" width=350 /><br>\n",
    "**Task:** Reshape the dataset `mnist_data` with dimensions `70000 x 28 x 28` to dimensions `70000 x 784`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### start ####\n",
    "mnist_reshape = ...\n",
    "### end ###\n",
    "\n",
    "print('New dimensions: {}'.format(mnist_reshape.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, the weights between the neurons of a neural network are initiated in a range between zero and one.  \n",
    "Now, if values between 0-255 (from our dataset so far) are given to the ANN as input, it must adjust to the range in the first steps of training. While this is possible, it unnecessarily increases training time and may make the neural network less reliable.\n",
    "\n",
    "**Task** Normalize the color values from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of the color values\n",
    "### start ###\n",
    "mnist_normalised = mnist_reshape / ...\n",
    "### end ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of our neural network is distinguishing between the ten different classes.\n",
    "This could be trained using a neuron with a linear activation function.\n",
    "However, it has been shown that classification works much better with 10 output neurons with logistic activation functions.\n",
    "Or even a **softmax** layer, where the output is normalized to a probability distribution over predicted output classes, based on [Luce's choice axiom](https://en.wikipedia.org/wiki/Luce%27s_choice_axiom)<br><br>\n",
    "In this example, the network is trained to generate the highest output at the neuron of the associated class.\n",
    "So the labels must be adapted with the so-called [One-Hot-Encoding](https://en.wikipedia.org/wiki/One-hot) in which a vector with integers becomes a matrix with zeros and ones.\n",
    "This is exemplary shown in the following picture.<br>\n",
    "\n",
    "<img src=\"03_Images/To_Categorical.svg\" width=500 />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to a one-hot matrix\n",
    "label_one_hot = keras.utils.to_categorical(mnist_label, dtype =\"bool\") # Data type: Boolean. What is it? Why are we using it?\n",
    "\n",
    "#Display of the procedure\n",
    "[print(mnist_label[i], label_one_hot[i]) for i in range(10)]\n",
    "print('\\nDimensions of the new vector:',label_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into test and training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it is still necessary to divide the dataset into a test and an evaluation dataset. Our algorithm is trained on the training dataset. Then, the evaluation dataset is predicted with the adapted neural network to evaluate the algorithm.<br><br>\n",
    "\n",
    "For this the already existing function `train_test_split` of `Scikit-Learn` is first imported and executed.<br>\n",
    "The data set and the labels are specified as arguments of the function. Afterwards it is defined how many percent of the data are selected pseudo-randomly into the test data set. Furthermore, it is possible to define by specifying a `random_state` that the same numbers are selected each time this function is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_evaluation, y_train, y_evaluation = train_test_split(mnist_normalised, \n",
    "                                                    label_one_hot, \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=42)\n",
    "\n",
    "print('Dimensions of training data  :', X_train.shape)\n",
    "print('Dimensions of test data      :', X_evaluation.shape)\n",
    "print('Dimensions of training labels:', y_train.shape)\n",
    "print('Dimensions of test labels    :', y_evaluation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now prepared and the training of the neural network can begin. In the first experiment we will use a neural network with 64 neurons in the first layer and 32 neurons in the second layer, similar to the network shown. As the last layer we will use a layer with 10 neurons, so that each neuron indicates the class membership to one of the 10 classes. To evaluate this affiliation, we use a `softmax` activation function. \n",
    "\n",
    "<img src=\"03_Images/ANN_MNIST.svg\" width=500 />\n",
    "\n",
    "The *Keras* library of Tensorflow was already imported in the first cell and can now be used. \n",
    "With `keras.Sequential(...)` a sequential structure of the network is defined and any number of layers and network architectures can be defined between the brackets. To create a feed-forward network, which we have already described mathematically in the slides, we use a sequence of **Dense** layers. **Dense** layers define that each neuron of the previous layer is connected to each neuron of this layer via weights. These layers are defined with `keras.layer.Dense(n_neurons, activation='activation')`.<br>\n",
    "\n",
    "**Task** Define a sequential network with three layers, with `[64, 32, 10]` neurons and the activation functions `['relu', 'relu', 'softmax']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the model\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        ### start ###\n",
    "        keras.layers.Dense(..., activation='...',input_shape=(784,)),\n",
    "        keras.layers.Dense(..., activation='...'),\n",
    "        keras.layers.Dense(..., activation='...')\n",
    "        ### end ###\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start training, we need to configure the training process. This is done by defining the **three key factors** for the compilation step:<br>\n",
    "\n",
    "**The optimizer** We stick to a pretty good standard: the gradient-based optimizer called [`adam`](https://arxiv.org/abs/1412.6980). \n",
    "Many other [optimization algorithms](https://keras.io/losses/) are already implemented in Keras, which you can also take a look at.<br>\n",
    "\n",
    "**The metric**. Keras also implements [many metrics](https://keras.io/api/metrics/) to evaluate the algorithm. Since this is a classification problem, we use the output value of Keras to determine the accuracy (`accuracy`).<br>\n",
    "\n",
    "**The loss function**. Since we are using a softmax output layer, we will use the cross-entropy loss. Keras distinguishes between `binary_crossentropy` (2 classes) and `categorical_crossentropy` (>2 classes). Thus, we choose the latter.\n",
    "Further loss functions are listed at [Keras loss functions](https://keras.io/losses/). <br><br>\n",
    "\n",
    "**Task:** Define the compilation step of the neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation of the sequential model\n",
    "### start ###\n",
    "model.compile(loss='...', metrics=['...'], optimizer='...')\n",
    "### end ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training des Modells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a model in **Keras** consists only of running `model.fit()`. There are still some possible parameters here, but we will only specify four manually:\n",
    "\n",
    "**The training data** (images and label), commonly known as `X_train`,`y_train`.<br>\n",
    "**The number of epochs** (iterations over the entire data set) for which to train. we start with `20` epochs.<br>\n",
    "**The batch size** (number of images per gradient update) to be used during training. `1024` or `2048` are valid  values.But almost any other value is also possible.<br>\n",
    "**The test data split**: We have already divided our data into training and evaluation data. With the test data split, we now define that the algorithm for evaluating the training, randomly selects 20% from the training data. Thus, we keep the evaluation data set for objective evaluation of the algorithm.\n",
    "<br><br>\n",
    "\n",
    "**Task:** Complete the information and start the training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training of the model\n",
    "### start ###\n",
    "history = model.fit(X..., \n",
    "                    y..., \n",
    "                    epochs=...,\n",
    "                    batch_size=..., \n",
    "                    validation_split=0.2)\n",
    "### end ###\n",
    "\n",
    "\n",
    "# Saving the network\n",
    "path_to_model = 'keras_mnist.h5'\n",
    "model.save(path_to_model)\n",
    "print('The model was saved as: \"%s\"\\n' % path_to_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the training starts, the time per step and the current values for the *loss* and *accuracy* are displayed for the test and training set respectively. So it is easy to follow the progress of the training already during the training and to notice possible errors early. \n",
    "\n",
    "The algorithm is now trained and the model has been saved for further use with the command:`model.save('path')`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will display the training history. For this purpose we have written the training progress in the variable `history` in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load of the network\n",
    "path_to_model = 'keras_mnist.h5'\n",
    "model = keras.models.load_model(path_to_model)\n",
    "print('The model: \"%s\" was loaded\\n' % path_to_model)\n",
    "\n",
    "# Display of the training progress - precision\n",
    "plt.plot(history.history['accuracy'])# depending on Keras version also 'acc\n",
    "plt.plot(history.history['val_accuracy']) # depending on Keras version also 'val_acc'.\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Accuracy training set', 'Accuracy test set'], loc='lower right')\n",
    "\n",
    "# Display of the training progress - Loss\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Accuracy training set', 'Accuracy test set'], loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a sharp increase in the first two epochs, the accuracy of the training set converges to 100% and the loss converges to zero. Thus, further training will likely continue to optimize both values. The values of the test data set, on the other hand, appear to have already reached near optimum. This could be due to the small size of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the network to predict the part of the data that was split off for evaluation. This is made possible with the attribute `prediction = model.predict(X)` of the class `model`. As a result we get the output of the Softmax layer. An example is shown with the code of the cell under the task. <br>\n",
    "**Task:** Check the trained network with the `X_evaluation` dataset and change the index to test different results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### start ###\n",
    "prediction = ...\n",
    "### end ###\n",
    "\n",
    "index=15\n",
    "plt.imshow(X_evaluation[index].reshape(28,28))\n",
    "print(np.vstack((range(10),np.round(prediction[index],1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not necessary to check all values manually. With the function: `model.evaluate(X,y)` all predictions can be analyzed at once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_metrics = model.evaluate(X_evaluation, y_evaluation)\n",
    "\n",
    "print(\"Test Loss:     %.3f\"% loss_and_metrics[0])\n",
    "print(\"Test Accuracy: %.2f\"% (loss_and_metrics[1]*100)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is already relatively good. More exciting than looking at **all** predictions is to display only the wrongly predicted ones.<br>\n",
    "**Task:** Execute the following code several times to plot the wrong predictions. Note that you have to decide in which line `==` (equal) and `!=` (unequal) should be written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reversal of one-hot\n",
    "prediction_numeric = np.argmax(prediction, axis=1)\n",
    "y_eval_numeric = np.argmax(y_evaluation, axis=1)\n",
    "\n",
    "# Comparison of vectors\n",
    "### start ###\n",
    "incorrect_indices = np.where(prediction_numeric ... y_eval_numeric)[0]\n",
    "correct_indices = np.where(prediction_numeric ... y_eval_numeric)[0]\n",
    "### end ###\n",
    "\n",
    "print(len(correct_indices),\" correctly classified\")\n",
    "print(len(incorrect_indices),\" misclassified\")\n",
    "\n",
    "def plot_incorrect_samples():\n",
    "    # adapt figure size to accomodate 18 subplots\n",
    "\n",
    "    figure_evaluation = plt.figure(figsize=(10,7.5))\n",
    "\n",
    "    # plot 9 incorrect predictions\n",
    "    for i, incorrect in enumerate(np.random.choice(incorrect_indices,12)):\n",
    "        plt.subplot(3,4,i+1)\n",
    "        plt.imshow(X_evaluation[incorrect].reshape(28,28))\n",
    "        plt.title(\"Predicted {}, Truth: {}\".format(prediction_numeric[incorrect], y_eval_numeric[incorrect]))\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "plot_incorrect_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some of these wrong predictions, it is understandable that they were not recognized. But most of the images were recognized correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusions Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn offers the possibility to generate a Confusions Matrix. In this matrix the predictions are compared with the true values. On the diagonal is the accuracy with which the respective number is predicted. Next to the diagonal is the percentage of incorrectly predicted values.\n",
    "\n",
    "**Task:** Plot the confusion matrix with the function: `plot = plot_confusion_matrix(y_true, y_pred, normalize=True)`. In doing so, set the correct variables for `y_true` and `y_pred`. What can you deduce from this graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### start ###\n",
    "...\n",
    "### end ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the networks with a picture of numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a model that can predict with a relatively high accuracy the numbers between 0-9.<br>\n",
    "With a bit of image recognition, you can recognize patterns on a solid color background and process this picture as inputs to a neural network. <br>\n",
    "For this purpose, we have written and provided a script for pattern recognition and subsequent classification. This script can be executed via the function `classify_image(path_to_image, path_to_model)`.<br> Here, `path_to_image` is the path to an image with text and `path_to_model` is the path to the model we just trained.\n",
    "\n",
    "**Task** Evaluate your network with the pictures `'02_Test-images/n1.jpg'`,`'02_Test-images/n2.jpg'`and `'02_Test-Pictures/n3.jpg'`<br>\n",
    "**Reminder** add the MLE-folder like: `'MLE-School/02_Test-images/n1.jpg'` when using **COLAB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### start ###\n",
    "...\n",
    "...\n",
    "...\n",
    "### end ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization of the network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations**, <br>\n",
    "The foundation stone has been laid and you have trained an artificial neural network that is capable of recognizing written digits. Thus, you have laid the foundation for a program for text recognition, such as provided by most smartphones.<br><br>\n",
    "**But** the system is not perfect yet and you can start now to optimize the parameters to further improve the prediction.\n",
    "\n",
    "**Task:** Vary the parameters already introduced above and evaluate the impact on the precision of the result. The following headings will guide you through the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of network V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the network architecture, you should introduce two important changes. On the one hand, the number of layers and neurons can be increased and a regularization technique should be implemented. \n",
    "\n",
    "**Customize the architecture**<br>\n",
    "Create a Sequential Network analogous to the first experiment and increase the number of layers and also neurons per layer. <br>**Attention** This significantly increases the training time. So compare the number of parameters that can be optimized before training.<br><br>\n",
    "**Regularization** <br>\n",
    "In order to train large networks, many methods have been developed to prevent over-fitting and to regularize the network.\n",
    "In the training of neural networks, the weights are initialized randomly at the beginning. By the gradient descent method, these are now optimized. Here it can happen that the prediction of the network is based on a few weights, which were randomly in a good range at the beginning of the training. Thus, during the training steps, these weights become more and more \"important\" and many other weights and neurons are \"deactivated\". To prevent this, among other things, the method *DropOut* was developed, in which a random number of neurons are deactivated in each training step. Thus, the network is forced to develop redundancies and thus to generalize better. More robust networks are created. This method is easy to integrate by adding a line after the *Dense* layer with the command: `keras.layers.Dropout(p)`, where *p* is the proportion(0-1) of neurons that are randomly disabled per step. A common value is between 5-10%.\n",
    "***Caution***: Do not insert dropout after the output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the model\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        ### start ###\n",
    "        keras.layers.Dense(..., activation='...',input_shape=(784,)),\n",
    "        ...\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "        ### end ###\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilation V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial *learning rate* should also be adjusted for the new network. This also influences the achievable result. The calculation of an optimum is relatively complex. In the following code a *learning rate* of $\\eta$ = 0.005 was given, which fit well for our larger network. Depending on the architecture you have defined, this value may vary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### start ###\n",
    "opt = keras.optimizers.Adam(learning_rate=...)\n",
    "### end  ###Based on the NIST dataset, a dataset was created by [Kaggle](https://www.kaggle.com), which consists of 28x28 pixels like the MNIST dataset. So the workflow from preparation to training can be adopted here. If you are already done with optimizing the MNIST mesh, then use again the function: `az_data, az_label = load_data(az_path)`. The normalization of the data and the creation of the network can be done in the same way as in the previous task.\n",
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'], optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now follows the training process. Here, too, the number of training epochs can be adjusted/optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### start ###\n",
    "history = model.fit(X..., \n",
    "                    y..., \n",
    "                    epochs=...,\n",
    "                    batch_size=..., \n",
    "                    validation_split=0.2)\n",
    "### end ###\n",
    "\n",
    "# Saving the network\n",
    "path_to_model = 'keras_mnist_V2.h5'\n",
    "model.save(path_to_model)\n",
    "print('The model was saved as: \"%s\"\\n' % path_to_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogous to the first network, the following code saves the new network as `'keras_mnist_V2.h5'` and again gives the progression of precision and loss over the number of trained epochs. At the end, the evaluation is added to have all metrics at a glance. Execute the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load of the network\n",
    "path_to_model = 'keras_mnist.h5'\n",
    "model = keras.models.load_model(path_to_model)\n",
    "print('The model: \"%s\" was loaded\\n' % path_to_model)\n",
    "\n",
    "# Display of the training progress - precision\n",
    "plt.plot(history.history['accuracy'])# depending on Keras version also 'acc\n",
    "plt.plot(history.history['val_accuracy']) # depending on Keras version also 'val_acc'.\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Accuracy training set', 'Accuracy test set'], loc='lower right')\n",
    "\n",
    "# Display of the training progress - Loss\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Accuracy training set', 'Accuracy test set'], loc='lower right')\n",
    "\n",
    "# Evaluation\n",
    "loss_and_metrics = model.evaluate(X_evaluation, y_evaluation)\n",
    "\n",
    "print(\"Test Loss:     %.3f\"% loss_and_metrics[0])\n",
    "print(\"Test Accuracy: %.2f\"% (loss_and_metrics[1]*100)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, the precision has increased over the first result. If not, you should change the previous values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with the self-written images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our own images can also be evaluated with this newly trained network. <br>\n",
    "**Task:** To do this, copy the command from the one in **Section 1.10** and match the name of the saved model to the new saved mesh `'keras_mnist_V2.h5'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### start ###\n",
    "...\n",
    "### end ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of letters from A-Z (addition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the NIST dataset, a dataset was created by [Kaggle](https://www.kaggle.com), which consists of 28x28 pixels like the MNIST dataset. So the workflow from preparation to training can be adopted here. If you are already done with optimizing the MNIST mesh, then use again the function: `az_data, az_label = load_data(az_path)`. The normalization of the data and the creation of the network can be done in the same way as in the previous task.\n",
    "**Reminder** add the MLE-folder like: `'MLE-School/...` when using **COLAB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### start ###\n",
    "az_path = './01_Dataset/az_Data.npz'\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to the MNIST dataset, the distribution here is not very uniform. One possibility to adjust this would be the data augmentation. Here, the images are slightly rotated, shifted or scaled, for example, and are thus an unknown data point for the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Number, frequency_in_dataset = np.unique(az_label.flatten(), return_counts=True)\n",
    "plt.bar(Number, frequency_in_dataset)\n",
    "plt.xticks(Number, 'ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "plt.xlabel('Number in the image')\n",
    "plt.ylabel('Quantity in the data set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of the data and training of the networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogous to the previous examples, you can train an ANN on this dataset. For the evaluation we have also provided images:`'./02_Test-images/b1.jpg'`-`'./02_Test-images/b3.jpg'`.\n",
    "\n",
    "**Reminder** add the MLE-folder like: `'MLE-School/02_Test-images/n1.jpg'` when using **COLAB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### end ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a number line on the paper yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the scaling of the images is crucial, because the function 'Classify Images' uses the dimensions of the images to decide if the pattern found is too small or if it can be a letter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In recent years, Convolutional Neural Networks have emerged as state of the art, especially for image recognition. Without going into details, we have built a small CNN here. As you can see, the methodology of network definition is the same. Only now other types of layers are used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Square layout images\n",
    "X_train_Q = X_train.reshape(-1,28,28,1)\n",
    "X_eval_Q = X_evaluation.reshape(-1,28,28,1)\n",
    "\n",
    "#  Definition of the Convolutional neural Network\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28,1)),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(100, activation='relu', kernel_initializer='he_uniform'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "# compile model\n",
    "opt = keras.optimizers.SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Trianing\n",
    "model.fit(X_train_Q, y_train, epochs=10, batch_size=1024, verbose=1)\n",
    "\n",
    "\n",
    "# save the network\n",
    "path_to_model = 'keras_mnist_CNN.h5'\n",
    "model.save(path_to_model)\n",
    "print('The model was saved as: \"%s\"\\n' % path_to_model)\n",
    "\n",
    "# Display of the training progress - precision\n",
    "plt.plot(history.history['accuracy'])# depending on Keras version also 'acc'\n",
    "plt.plot(history.history['val_accuracy']) # depending on Keras version also 'val_acc'.\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Accuracy traing set', 'Accuracy test set'], loc='lower right')\n",
    "\n",
    "# Display of the training progress - Loss\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Loss training set', 'Loss test_set'], loc='upper right')\n",
    "\n",
    "# Evaluation\n",
    "loss_and_metrics = model.evaluate(X_eval_Q, y_evaluation)\n",
    "\n",
    "print(\"Test Loss:     %.3f\"% loss_and_metrics[0])\n",
    "print(\"Test Accuracy: %.2f\"% (loss_and_metrics[1]*100)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_model = 'keras_mnist_CNN.h5'\n",
    "\n",
    "classify_image('02_Test-images/n1.jpg', path_to_model, CNN=True)\n",
    "classify_image('02_Test-images/n2.jpg', path_to_model, CNN=True)\n",
    "classify_image('02_Test-images/n3.jpg', path_to_model, CNN=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN_az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Square layout images\n",
    "X_train_Q_az = X_train_az.reshape(-1,28,28,1)\n",
    "X_eval_Q_az = X_evaluation_az.reshape(-1,28,28,1)\n",
    "\n",
    "#  Definition of the Convolutional neural Network\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28,1)),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(100, activation='relu', kernel_initializer='he_uniform'),\n",
    "    keras.layers.Dense(36, activation='softmax')\n",
    "    ])\n",
    "# compile model\n",
    "opt = keras.optimizers.SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Training\n",
    "model.fit(X_train_Q, y_train, epochs=10, batch_size=1024, verbose=1)\n",
    "\n",
    "# save the network\n",
    "path_to_model = 'keras_az_CNN.h5'\n",
    "model.save(path_to_model)\n",
    "print('The model was saved as: \"%s\"\\n' % path_to_model)\n",
    "\n",
    "# Display of the training progress - precision\n",
    "plt.plot(history.history['accuracy'])# depending on Keras version also 'acc'\n",
    "plt.plot(history.history['val_accuracy']) # depending on Keras version also 'val_acc'.\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Accuracy traing set', 'Accuracy test set'], loc='lower right')\n",
    "\n",
    "# Display of the training progress - Loss\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Loss training set', 'Loss test_set'], loc='upper right')\n",
    "\n",
    "# Evaluation\n",
    "loss_and_metrics = model.evaluate(X_eval_Q, y_evaluation)\n",
    "\n",
    "print(\"Test Loss:     %.3f\"% loss_and_metrics[0])\n",
    "print(\"Test Accuracy: %.2f\"% (loss_and_metrics[1]*100)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classify_image('02_Test-images/b1.jpg', 'keras_az_CNN.h5', CNN=True)\n",
    "classify_image('02_Test-images/b2.jpg', 'keras_az_CNN.h5', CNN=True)\n",
    "classify_image('02_Test-images/b3.jpg', 'keras_az_CNN.h5', CNN=True)\n",
    "classify_image('02_Test-images/b4.jpg', 'keras_az_CNN.h5', CNN=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainable AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep_explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# select a set of background examples to take an expectation over\n",
    "background = X_train_Q[np.random.choice(X_train_Q.shape[0], 100, replace=False)]\n",
    "\n",
    "\n",
    "path_to_model = 'keras_mnist_CNN.h5'\n",
    "model = keras.models.load_model(path_to_model)\n",
    "\n",
    "# explain predictions of the model on three images\n",
    "e = shap.DeepExplainer(model, background)\n",
    "# ...or pass tensors directly\n",
    "# e = shap.DeepExplainer((model.layers[0].input, model.layers[-1].output), background)\n",
    "shap_values = e.shap_values(X_eval_Q[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cover Picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_image('02_Test-images/MLE.jpg', 'keras_az_CNN.h5', True)\n",
    "classify_image('02_Test-images/MLEz.jpg', 'keras_mnist_CNN.h5', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time measurements other algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the memory used, another package is needed. To install it, you have to change the following line to a code line and execute it once..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!conda install -c anaconda memory_profiler -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modules Loading and Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "%load_ext memory_profiler\n",
    "\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Test Train\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist_normalised, mnist_label.ravel(), test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC\n",
    "model = SVC(kernel='linear')\n",
    "\n",
    "#training\n",
    "%time %memit model.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "print('prediction')\n",
    "%time %memit y_pred = model.predict(X_test)\n",
    "# accuracy\n",
    "print(\"accuracy:\", metrics.accuracy_score(y_true=y_test, y_pred=y_pred), \"\\n\")\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NuSVC\n",
    "model = NuSVC()\n",
    "\n",
    "#training\n",
    "%time %memit model.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "print('prediction')\n",
    "%time %memit y_pred = model.predict(X_test)\n",
    "# accuracy\n",
    "print(\"accuracy:\", metrics.accuracy_score(y_true=y_test, y_pred=y_pred), \"\\n\")\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "model = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "#training\n",
    "%time %memit model.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "print('prediction')\n",
    "%time %memit y_pred = model.predict(X_test)\n",
    "# accuracy\n",
    "print(\"\\naccuracy:\", metrics.accuracy_score(y_true=y_test, y_pred=y_pred), \"\\n\")\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extra Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Tree\n",
    "model = ExtraTreeClassifier(random_state=0)\n",
    "\n",
    "#training\n",
    "%time %memit model.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "print('prediction')\n",
    "%time %memit y_pred = model.predict(X_test)\n",
    "# accuracy\n",
    "print(\"\\naccuracy:\", metrics.accuracy_score(y_true=y_test, y_pred=y_pred), \"\\n\")\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forrest\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "#training\n",
    "%time %memit model.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "print('prediction')\n",
    "%time %memit y_pred = model.predict(X_test)\n",
    "# accuracy\n",
    "print(\"\\naccuracy:\", metrics.accuracy_score(y_true=y_test, y_pred=y_pred), \"\\n\")\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### scikit learn MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "model = MLPClassifier(hidden_layer_sizes=(64,32,), max_iter=10, alpha=1e-4,\n",
    "                    solver='sgd', random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "\n",
    "#training\n",
    "start = time.time()\n",
    "%time %memit model.fit(X_train, y_train)\n",
    "print('actual time:', time.time()-start)\n",
    "# predict\n",
    "print('prediction')\n",
    "%time %memit y_pred = model.predict(X_test)\n",
    "# accuracy\n",
    "print(\"\\naccuracy:\", metrics.accuracy_score(y_true=y_test, y_pred=y_pred), \"\\n\")\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST_Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18,9))\n",
    "im = np.zeros((1,28*20+1))\n",
    "for i in range(10):\n",
    "    itemindex = np.where(mnist_label.reshape(70000,)==i)[0]\n",
    "    row = np.zeros((28,1))\n",
    "    for k in range(20):\n",
    "        row = np.hstack((row, mnist_data[itemindex[k]]))\n",
    "    im = np.vstack((im, row))\n",
    "plt.imshow(im, cmap='binary')\n",
    "fig.patch.set_visible(False)\n",
    "ax.axis('off')\n",
    "plt.savefig('Beispiel1.jpg', dpi=300, bbox_inches='tight', pad_inches=0.1, frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18,9))\n",
    "im = np.zeros((28*11+1,1))\n",
    "for i in range(10):\n",
    "    itemindex = np.where(mnist_label.reshape(70000,)==i)[0]\n",
    "    row = np.zeros((1,28))\n",
    "    for k in range(11):\n",
    "        row = np.vstack((row, mnist_data[itemindex[k]]))\n",
    "    im = np.hstack((im, row))\n",
    "plt.imshow(im, cmap='binary')\n",
    "fig.patch.set_visible(False)\n",
    "ax.axis('off')\n",
    "plt.savefig('Beispiel2.jpg', dpi=300, bbox_inches='tight', pad_inches=0.1, frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(mnist_data[0], cmap='binary', interpolation='none')\n",
    "plt.title(\"Zahl: {}\".format(mnist_label[0][0]))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.savefig('Beispiel3.png', dpi=600, bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for number in range(15):\n",
    "    new_im = []\n",
    "    n = 10\n",
    "    for row in mnist_data[number]:\n",
    "        new_row= []\n",
    "        for pix in row:\n",
    "            new_row.append(255)\n",
    "            for i in range(n):\n",
    "                new_row.append(pix)\n",
    "        new_row.append(255)\n",
    "        for i in range(n):\n",
    "            new_im.append(new_row)\n",
    "        new_im.append(np.ones((len(new_row)))*255)\n",
    "\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(new_im, cmap='binary', interpolation='none')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.savefig('Nummer_%i.png'%number, dpi=600, bbox_inches='tight', pad_inches=0.1)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf29')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "275px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "08e3ed9748c573707b34472c59d1a8c58e1cdea5d49f0b477cad1c7731bc75cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
